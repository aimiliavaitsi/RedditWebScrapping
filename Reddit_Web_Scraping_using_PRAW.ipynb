{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install praw openpyxl aiohttp nest_asyncio\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import logging\n",
        "import time\n",
        "from google.colab import files\n",
        "from praw.models import MoreComments\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Suppress PRAW warnings\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "\n",
        "# Initialize Reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"REDDIT_CLIENT_ID",\n",
        "    client_secret=\"REDDIT_CLIENT_SECRET",\n",
        "    user_agent=\"REDDIT_USER_AGENT",\n",
        ")\n",
        "\n",
        "# Define search parameters\n",
        "queries = [\n",
        "    \"biomaterial\", \"biodegradation\", \"biopolymers\", \"bioplastic\", \"biodegradable polymer\",\n",
        "    \"bioplastics\", \"polyhydroxyalkanoates\", \"biodegradable plastic\", \"biodegradable materials\",\n",
        "    \"bio-composite\", \"bio-based plastic\", \"compostable plastic\", \"bio-plastic\",\n",
        "    \"sustainable polymer\", \"green plastic\", \"sustainable plastic\", \"PLA plastic\",\n",
        "    \"microbial plastic\", \"renewable plastic\", \"starch-based plastic\", \"enzyme degradation\",\n",
        "    \"natural plastic\", \"eco-friendly plastic\", \"bio-derived plastic\", \"marine biodegradable plastic\",\n",
        "    \"plant-based plastic\", \"algae-based plastic\", \"PHA plastic\", \"polyactic acid\",\n",
        "    \"cellulose-based plastic\", \"lignin-based plastic\", \"chitosan plastic\"\n",
        "]\n",
        "subreddits = [\"all\", \"science\", \"environment\", \"technology\", \"sustainable\"]\n",
        "\n",
        "# Targeted years\n",
        "targeted_years = [(2010, 2013), (2024, 2024)]\n",
        "\n",
        "# General search start and end dates\n",
        "general_start_date = datetime.datetime(2010, 1, 1)\n",
        "general_end_date = datetime.datetime(2024, 1, 1)\n",
        "\n",
        "# List to store comments and dates\n",
        "comments_data = []\n",
        "\n",
        "# Function to check if a post contains any of the keywords\n",
        "def contains_keyword(text, keywords):\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in keywords)\n",
        "\n",
        "# Function to fetch comments\n",
        "async def fetch_comments(comment_list, submission_title, submission_text, submission_url, submission_created_utc):\n",
        "    for comment in comment_list:\n",
        "        if isinstance(comment, MoreComments):\n",
        "            continue\n",
        "        comments_data.append([submission_title, submission_text, comment.body, submission_url, comment.created_utc])\n",
        "        await fetch_comments(comment.replies, submission_title, submission_text, submission_url, submission_created_utc)\n",
        "\n",
        "# Function to search submissions and fetch comments\n",
        "async def search_and_fetch_comments(query, subreddit, start_timestamp, end_timestamp, session, semaphore):\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            submissions = reddit.subreddit(subreddit).search(query, sort=\"new\", time_filter=\"all\", limit=1000)\n",
        "            for submission in submissions:\n",
        "                if start_timestamp <= submission.created_utc <= end_timestamp and contains_keyword(submission.title, queries):\n",
        "                    submission.comments.replace_more(limit=None)\n",
        "                    await fetch_comments(submission.comments, submission.title, submission.selftext, submission.url, submission.created_utc)\n",
        "                    await asyncio.sleep(2)  # Delay between requests to handle rate limiting\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            await asyncio.sleep(60)  # Sleep for 1 minute before retrying\n",
        "\n",
        "# Perform searches for multiple queries and subreddits\n",
        "async def main(targeted=True):\n",
        "    tasks = []\n",
        "    semaphore = asyncio.Semaphore(10)  # Limit concurrent requests\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for query in queries:\n",
        "            for subreddit in subreddits:\n",
        "                if targeted:\n",
        "                    for start_year, end_year in targeted_years:\n",
        "                        start_date = datetime.datetime(start_year, 1, 1)\n",
        "                        end_date = datetime.datetime(end_year, 12, 31)\n",
        "                        start_timestamp = int(start_date.timestamp())\n",
        "                        end_timestamp = int(end_date.timestamp())\n",
        "                        tasks.append(search_and_fetch_comments(query, subreddit, start_timestamp, end_timestamp, session, semaphore))\n",
        "                        await asyncio.sleep(5)  # Delay between searches for different queries and subreddits\n",
        "                else:\n",
        "                    start_timestamp = int(general_start_date.timestamp())\n",
        "                    end_timestamp = int(general_end_date.timestamp())\n",
        "                    tasks.append(search_and_fetch_comments(query, subreddit, start_timestamp, end_timestamp, session, semaphore))\n",
        "                    await asyncio.sleep(5)  # Delay between searches for different queries and subreddits\n",
        "\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "# Run targeted year search first\n",
        "await main(targeted=True)\n",
        "\n",
        "# General search for other years\n",
        "await main(targeted=False)\n",
        "\n",
        "# Create DataFrame\n",
        "comments_df = pd.DataFrame(comments_data, columns=[\"Post Title\", \"Post Text\", \"Comment\", \"URL\", \"Date\"])\n",
        "\n",
        "# Convert the date from Unix timestamp to readable format\n",
        "comments_df[\"Date\"] = pd.to_datetime(comments_df[\"Date\"], unit='s')\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_file = \"/content/reddit_data.xlsx\"\n",
        "comments_df.to_excel(excel_file, index=False)\n",
        "\n",
        "# Download the Excel file in Google Colab\n",
        "files.download(excel_file)\n"
      ],
      "metadata": {
        "id": "EOZZfmp-OZJP",
        "outputId": "0b747920-0332-471e-964d-6b90bc8cd030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    }
  ]
}
